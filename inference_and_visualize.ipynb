{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from models import build_DABDETR, build_dab_deformable_detr\n",
    "from util.slconfig import SLConfig\n",
    "from datasets import build_dataset\n",
    "from util.visualizer import COCOVisualizer\n",
    "from util import box_ops\n",
    "\n",
    "from PIL import Image\n",
    "import datasets.transforms as T\n",
    "import requests, io\n",
    "\n",
    "\n",
    "vslzr = COCOVisualizer()\n",
    "\n",
    "def dtd(d,device):\n",
    "    return {k: v.to(device=device, non_blocking=True) if hasattr(v, 'to') \n",
    "               else dtd(v,device) if hasattr(v, 'items') \n",
    "               else [dtd(i,device) for i in v] if isinstance(v,list)\n",
    "               else v for k, v in d.items()}\n",
    "model_config_path = \"model_zoo/DAB_DETR/R50_v2/config.json\" # change the path of the model config file\n",
    "model_checkpoint_path = \"model_zoo/DAB_DETR/R50_v2/checkpoint.pth\" # change the path of the model checkpoint\n",
    "# See our Model Zoo section in README.md for more details about our pretrained models.\n",
    "\n",
    "device='cpu'\n",
    "args = SLConfig.fromfile(model_config_path) \n",
    "model, criterion, postprocessors = build_dab_deformable_detr(args)\n",
    "checkpoint = torch.load(model_checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model = model.to('cuda')\n",
    "def dtd(d,device):\n",
    "    return {k: v.to(device=device, non_blocking=True) if hasattr(v, 'to') \n",
    "               else dtd(v,device) if hasattr(v, 'items') \n",
    "               else [dtd(i,device) for i in v] if isinstance(v,list)\n",
    "               else v for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(url):\n",
    "\n",
    "\n",
    "    response = requests.get(url)\n",
    "    image_bytes = io.BytesIO(response.content)\n",
    "    image_init = Image.open(image_bytes).convert(\"RGB\")\n",
    "\n",
    "\n",
    "    # image\n",
    "    # transform images\n",
    "    transform = T.Compose([\n",
    "        T.RandomResize([800], max_size=1333),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image, _ = transform(image_init, None)\n",
    "    # predict images\n",
    "    output = model(image[None].to('cuda'))\n",
    "    output = dtd(output,'cpu')\n",
    "    output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]))[0]\n",
    "    # visualize outputs\n",
    "    thershold = 0.35 # set a thershold\n",
    "\n",
    "    scores = output['scores']\n",
    "    labels = output['labels']\n",
    "    boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
    "    select_mask = scores > thershold\n",
    "\n",
    "    # box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
    "    pred_dict = {\n",
    "        'boxes': boxes[select_mask],\n",
    "        'size': torch.Tensor([image.shape[1], image.shape[2]]),\n",
    "        # 'box_label': box_label\n",
    "    }\n",
    "    # vslzr.visualize(image, pred_dict, savedir=None)\n",
    "    W,H = image_init.size\n",
    "\n",
    "    images = []\n",
    "\n",
    "    for box in pred_dict['boxes']:\n",
    "        unnormbbox = box * torch.Tensor([W, H, W, H])\n",
    "        unnormbbox[:2] -= unnormbbox[2:] / 2\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = unnormbbox.tolist()\n",
    "        images.append(image_init.crop((bbox_x,bbox_y,bbox_x+bbox_w,bbox_y+bbox_h)))\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://image.shutterstock.com/z/stock-photo-club-sandwich-and-french-fries-on-a-light-wooden-board-next-to-the-potatoes-is-a-cup-of-ketchup-1147730354.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/DAB-DETR/models/dab_deformable_detr/position_encoding.py:53: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
      "/home/dima/anaconda3/envs/mis/lib/python3.9/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/dima/DAB-DETR/models/dab_deformable_detr/deformable_transformer.py:489: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = 10000 ** (2 * (dim_t // 2) / 128)\n",
      "/home/dima/DAB-DETR/models/dab_deformable_detr/dab_deformable_detr.py:463: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  topk_boxes = topk_indexes // out_logits.shape[2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=297x315 at 0x7FA5A3FBD5E0>,\n",
       " <PIL.Image.Image image mode=RGB size=302x321 at 0x7FA5A3FBDFA0>,\n",
       " <PIL.Image.Image image mode=RGB size=307x296 at 0x7FA74C137400>,\n",
       " <PIL.Image.Image image mode=RGB size=273x300 at 0x7FA74C137310>,\n",
       " <PIL.Image.Image image mode=RGB size=216x216 at 0x7FA74C137A00>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_boxes(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "85413491e356ed13695e9bf9554aa8eea2265282f19b28c8f8dbc9d7d846f3e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
